{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_selfplay",
      "provenance": [],
      "collapsed_sections": [
        "JFJjCfnOHicf"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1rQvvsEm_a15",
        "outputId": "77470ab1-773f-4da2-9642-cf1bdecde2cd"
      },
      "source": [
        "## Install packages\n",
        "!pip install torch_geometric\n",
        "!pip install torch_sparse\n",
        "!pip install torch_scatter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.0.0.tar.gz (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (4.62.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.6.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.0-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 41.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (2.4.7)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch_geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch_geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch_geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch_geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch_geometric) (57.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch_geometric) (2021.5.30)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch_geometric) (1.0.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.0-py3-none-any.whl size=513273 sha256=214e73ac591b3508796c41fc51a2cfc77eb40620d7b860b0c8a61b4b3f2fecd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/22/3f/10c97635d8d73e012582116b4e449d47fc64b7246cc71085fc\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.0 torch-geometric-2.0.0 yacs-0.1.8\n",
            "Collecting torch_sparse\n",
            "  Downloading torch_sparse-0.6.12.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 749 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch_sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch_sparse) (1.19.5)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl size=510307 sha256=7c2de95a9028b1be967959c2dfe549c17b3597928c7e227a3bffae7d58adeab5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/e2/2f/44956c61e3299573ffe12da9d1374c7576ca0c5fb1fe1ed38c\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Collecting torch_scatter\n",
            "  Downloading torch_scatter-2.0.8.tar.gz (21 kB)\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.0.8-cp37-cp37m-linux_x86_64.whl size=281098 sha256=722acc9f1e7241762c8470441685d37c869071362d523da19a3a5ef53eda2b3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/e4/4e/2bcc6de6a801960aedbca43f7106d268f766c3f9f8ab49b3a5\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvdMUNgJ_uyN"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU\n",
        "from torch import Tensor\n",
        "from torch.utils.dlpack import to_dlpack, from_dlpack\n",
        "\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "# # import multiwoz.conv_graph\n",
        "# from multiwoz.conv_graph import MultiWozConvGraph\n",
        "from GNN_untils_test import evaluate_model, LSTM_model, Perceptron, BiLSTM, RNN_model\n",
        "from GNN_untils_test import SoftBCEWithLogitsLoss, validate_model, f1\n",
        "from GNN_untils_test import CreateDatalist, GAT_emb3, SageModel3, MLPClassifier, validate_model_GNN, evaluate_model_GNN\n",
        "\n",
        "\n",
        "# from conv_graph_gnn import MultiWozConvGraph_GNN\n",
        "from conv_graph_selfplay_gnn import SelfPlayConvGraph\n",
        "# from utils_and_torch import evaluate_model, Classifier\n",
        "# from utils_and_torch import SoftBCEWithLogitsLoss, validate_model, f1\n",
        "\n",
        "import networkx as nx\n",
        "import torch_geometric.data\n",
        "from torch_geometric.data import InMemoryDataset, Data, Batch\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, degree\n",
        "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv, SAGEConv\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GATConv"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oQFbPRgv3a8",
        "outputId": "ea6a642e-1091-4008-f87e-62ed398d7339"
      },
      "source": [
        "## Generate Graphs\n",
        "net_depth = 2\n",
        "train_graph_GNN = SelfPlayConvGraph(dir_name=\"./\", file_names=['/train.json'], seq_length = 4, net_depth = net_depth)\n",
        "dev_graph_GNN = SelfPlayConvGraph(dir_name=\"./\", file_names=['/dev.json'], seq_length = 4, net_depth = net_depth)\n",
        "test_graph_GNN = SelfPlayConvGraph(dir_name=\"./\", file_names=['/test.json'], seq_length = 4, net_depth = net_depth)\n",
        "eval_graph_GNN = SelfPlayConvGraph(dir_name=\"./\", file_names=['/train.json', '/dev.json', '/test.json'], seq_length = 4, net_depth = net_depth)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//train.json\n",
            "Average degree: 2.013 (excluding outliers)\n",
            "Number of nodes: 472\n",
            "Number of edges: 1002\n",
            "Number of conversations: 384\n",
            "Unique turns: 978\n",
            "Total turns: 3562\n",
            "As a percentage: 27.456\n",
            "-----------------------------------------------\n",
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//dev.json\n",
            "Average degree: 1.857 (excluding outliers)\n",
            "Number of nodes: 279\n",
            "Number of edges: 518\n",
            "Number of conversations: 120\n",
            "Unique turns: 498\n",
            "Total turns: 1134\n",
            "As a percentage: 43.915\n",
            "-----------------------------------------------\n",
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//test.json\n",
            "Average degree: 1.820 (excluding outliers)\n",
            "Number of nodes: 436\n",
            "Number of edges: 857\n",
            "Number of conversations: 264\n",
            "Unique turns: 842\n",
            "Total turns: 2464\n",
            "As a percentage: 34.172\n",
            "-----------------------------------------------\n",
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//train.json and /dev.json and /test.json\n",
            "Average degree: 2.240 (excluding outliers)\n",
            "Number of nodes: 619\n",
            "Number of edges: 1491\n",
            "Number of conversations: 768\n",
            "Unique turns: 1458\n",
            "Total turns: 7160\n",
            "As a percentage: 20.363\n",
            "-----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMtKm_zuwDcX",
        "outputId": "9b6a59cb-07ab-4da2-9acd-97c24153ba23"
      },
      "source": [
        "## GNN subgraphs data\n",
        "x_train_GNN, y_train_GNN, edge_index_train, x_train_simple = train_graph_GNN.generate_subgraph_data()\n",
        "x_dev_GNN, y_dev_GNN, edge_index_dev, x_dev_simple = dev_graph_GNN.generate_subgraph_data()\n",
        "x_test_GNN, y_test_GNN, edge_index_test, x_test_simple = test_graph_GNN.generate_subgraph_data()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//train.json\n",
            "Subgraph list created successfully.\n",
            "Number of subgraphs created: 1589\n",
            "-----------------------------------------------\n",
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//dev.json\n",
            "Subgraph list created successfully.\n",
            "Number of subgraphs created: 507\n",
            "-----------------------------------------------\n",
            "-----------------------------------------------\n",
            "Stats for ConvGraph for .//test.json\n",
            "Subgraph list created successfully.\n",
            "Number of subgraphs created: 1100\n",
            "-----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fusPjzNIlhc3",
        "outputId": "e5cd08d7-8f94-48e7-8b29-5a67ec49ded5"
      },
      "source": [
        "print(len(x_train_GNN))\n",
        "print(len(x_train_GNN[1][-1]))\n",
        "print(len(y_train_GNN[1]))\n",
        "\n",
        "## x of length 31\n",
        "## y of length 12"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1589\n",
            "31\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JqorPn8gyVy",
        "outputId": "231413de-ff11-4511-bc6b-23cca152bfe2"
      },
      "source": [
        "max_num_nodes_train = max([len(x_train_GNN[i]) for i in range(len(x_train_GNN))])\n",
        "max_num_nodes_test = max([len(x_test_GNN[i]) for i in range(len(x_test_GNN))])\n",
        "max_num_nodes_dev = max([len(x_dev_GNN[i]) for i in range(len(x_dev_GNN))])\n",
        "\n",
        "print(max_num_nodes_train, max_num_nodes_test, max_num_nodes_dev)\n",
        "\n",
        "min_num_nodes_train = min([len(x_train_GNN[i]) for i in range(len(x_train_GNN))])\n",
        "min_num_nodes_test = min([len(x_test_GNN[i]) for i in range(len(x_test_GNN))])\n",
        "min_num_nodes_dev = min([len(x_dev_GNN[i]) for i in range(len(x_dev_GNN))])\n",
        "\n",
        "print(min_num_nodes_train, min_num_nodes_test, min_num_nodes_dev)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56 56 35\n",
            "2 2 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ97CCPuhLKT",
        "outputId": "38baa03a-adaa-4238-be3c-c5724fcc4d62"
      },
      "source": [
        "## Generate test data for simple search in search_graph\n",
        "train_graph = train_graph_GNN.graph\n",
        "# search_graph = train_dev_graph.graph\n",
        "x_test = [x_test_GNN[i][-1] for i in range(len(x_test_GNN))]\n",
        "# x_test_unique = np.unique(x_test, axis = 0)\n",
        "\n",
        "x_dev = [x_dev_GNN[i][-1] for i in range(len(x_dev_GNN))]\n",
        "# x_dev_unique = np.unique(x_dev, axis = 0)\n",
        "\n",
        "x_seen_train = [x_train_simple[i][-1] for i in range(len(x_train_simple))]\n",
        "x_seen_dev = [x_dev_GNN[i][-1] for i in range(len(x_dev_GNN))]\n",
        "x_seen = np.concatenate((x_seen_train, x_seen_dev))\n",
        "\n",
        "print('Simple search data created successfully.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple search data created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXyFquFHEysE"
      },
      "source": [
        "### Create graph data\n",
        "Each x_i in the dataset is a set of nodes for one individual graph. Every graph has different number of nodes N_i with shape [N_i, 31]. \n",
        "\n",
        "Each graph has one label y_i of shape [12]. The aim is to predict the label for a given graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8OVVE7MF6zj"
      },
      "source": [
        "def CreateDatalist(x_data, y_data, edge_data):\n",
        "    data_list = []\n",
        "        \n",
        "    num_data = len(x_data)\n",
        "        \n",
        "    for i in range(num_data):\n",
        "        x = torch.tensor(x_data[i])\n",
        "        y = torch.tensor(y_data[i])\n",
        "        edge_index = torch.tensor(edge_data[i]).t().contiguous()\n",
        "            \n",
        "        data = Data(x = x, y = y, edge_index = edge_index)\n",
        "        data_list.append(data)\n",
        "    return data_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X-AYKE4Gr3q"
      },
      "source": [
        "train_datalist = CreateDatalist(x_train_GNN, y_train_GNN, edge_index_train)\n",
        "dev_datalist = CreateDatalist(x_dev_GNN, y_dev_GNN, edge_index_dev)\n",
        "test_datalist = CreateDatalist(x_test_GNN, y_test_GNN, edge_index_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS0uoCStGuUc",
        "outputId": "3c68522a-497d-4266-9862-00d0124630c9"
      },
      "source": [
        "## Batch data droplast\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_datalist, batch_size = batch_size,drop_last=True)\n",
        "dev_loader = DataLoader(dev_datalist, batch_size = batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_datalist, batch_size = batch_size,drop_last=True)\n",
        "\n",
        "print(\"Batch created successfully.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch created successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrelFOBAHaHH"
      },
      "source": [
        "## Testrun batch\n",
        "testrun_batch = next(iter(train_loader))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnj0Hz3OHDe1"
      },
      "source": [
        "### Graph attention model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8cEAdjG6ho"
      },
      "source": [
        "## Three layers of attention for three layers of neighbours\n",
        "class GAT_emb(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GAT_emb, self).__init__()\n",
        "        \n",
        "        self.conv1 = GATConv(in_channels, 8, heads=1)\n",
        "        self.conv2 = GATConv(8, 8, heads=1, concat=False)\n",
        "        self.conv3 = GATConv(8, out_channels, heads=1, concat=False)\n",
        "                \n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.elu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.log_softmax(x, dim=-1)\n",
        "        \n",
        "        ## global mean pooling\n",
        "        x = gmp(x, data.batch)  ## max pooling\n",
        "        # x = gap(x, data.batch) ## mean pooling\n",
        "\n",
        "        # x = x[-1]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umiZLjnkjePf",
        "outputId": "8a82e2e9-46d2-41a8-e58c-384b3f90c930"
      },
      "source": [
        "## Test run\n",
        "model = GAT_emb(31,31)\n",
        "temp = model(testrun_batch)\n",
        "\n",
        "print(temp.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEo72lB8HJKL"
      },
      "source": [
        "### GraphSage model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgU4Z8EIxwox"
      },
      "source": [
        "## Three layers of sage for three layers of neighbours\n",
        "class SageModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, dim_hidden_sage, out_channels):\n",
        "        super(SageModel,self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, dim_hidden_sage)\n",
        "        self.conv2 = SAGEConv(dim_hidden_sage, 32) \n",
        "        self.conv3 = SAGEConv(32, out_channels)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        ## global mean pooling\n",
        "        # x = gap(x, data.batch)\n",
        "        x = gmp(x, data.batch)\n",
        "        \n",
        "        return x\n",
        "        "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gq2aA6MyQUm",
        "outputId": "a4f60ab8-aae7-4599-a4c6-8cf512e9f553"
      },
      "source": [
        "## Test run\n",
        "model = SageModel(31, 64, 31)\n",
        "temp = model(testrun_batch)\n",
        "\n",
        "print(temp.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-ngreUXWHNBT"
      },
      "source": [
        "#@title\n",
        "# class SAGEConv(MessagePassing):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(SAGEConv, self).__init__(aggr='mean') #  \"mean\" aggregation.\n",
        "#         self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "#         self.act = torch.nn.ReLU()\n",
        "#         self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n",
        "#         self.update_act = torch.nn.ReLU()\n",
        "        \n",
        "        \n",
        "#     def forward(self, data):\n",
        "#         '''\n",
        "#         x: [N, in_channels]\n",
        "#         edge_index: [2, E]\n",
        "#         x_j: [E, in_channels]\n",
        "#         aggr_out: [N, out_channels]\n",
        "        \n",
        "#         '''  \n",
        "#         x = data.x.float()\n",
        "#         edge_index = data.edge_index\n",
        "        \n",
        "#         edge_index, _ = remove_self_loops(edge_index)\n",
        "#         edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) \n",
        "        \n",
        "#         return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
        "\n",
        "#     def message(self, x_j):\n",
        "#         x_j = self.lin(x_j)\n",
        "#         x_j = self.act(x_j)\n",
        "        \n",
        "#         return x_j\n",
        "\n",
        "#     def update(self, aggr_out, x):\n",
        "#         new_embedding = torch.cat([aggr_out, x], dim=1)\n",
        "        \n",
        "#         new_embedding = self.update_lin(new_embedding)\n",
        "#         new_embedding = self.update_act(new_embedding)\n",
        "        \n",
        "#         return new_embedding"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfga-IT774SH"
      },
      "source": [
        "### Depth 2 GNN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dRbbHxe77cE"
      },
      "source": [
        "## Three layers of attention for three layers of neighbours\n",
        "class GAT_emb(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GAT_emb, self).__init__()\n",
        "        \n",
        "        self.conv1 = GATConv(in_channels, 8, heads=1)\n",
        "        self.conv2 = GATConv(8, out_channels, heads=1, concat=False)\n",
        "                \n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.log_softmax(x, dim=-1)\n",
        "        \n",
        "        ## global mean pooling\n",
        "        # x = gap(x, data.batch)\n",
        "        x = gmp(x, data.batch)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "979ab46eC72-",
        "outputId": "209566fe-92bc-4384-8526-ebcfccfa0227"
      },
      "source": [
        "## Test run\n",
        "model = GAT_emb(31,31)\n",
        "temp = model(testrun_batch)\n",
        "\n",
        "print(temp.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xTAS50zC_2r"
      },
      "source": [
        "## Three layers of sage for three layers of neighbours\n",
        "class SageModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, dim_hidden_sage, out_channels):\n",
        "        super(SageModel,self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, dim_hidden_sage)\n",
        "        self.conv2 = SAGEConv(dim_hidden_sage, out_channels)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        ## global mean pooling\n",
        "        x = gap(x, data.batch)\n",
        "        \n",
        "        return x\n",
        "        "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nP_Dh9vDCZs",
        "outputId": "92038e12-6b23-4d49-da3b-3c368a727d72"
      },
      "source": [
        "## Test run\n",
        "model = SageModel(31, 64, 31)\n",
        "temp = model(testrun_batch)\n",
        "\n",
        "print(temp.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Gw2UxuQzAg"
      },
      "source": [
        "### Depth 1 GNN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeg-fSBAQyTL"
      },
      "source": [
        "## Three layers of attention for three layers of neighbours\n",
        "class GAT_emb(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GAT_emb, self).__init__()\n",
        "        \n",
        "        self.conv1 = GATConv(in_channels, out_channels, heads=2, concat=False)\n",
        "                \n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.log_softmax(x, dim=-1)\n",
        "        \n",
        "        ## global mean pooling\n",
        "        # x = gap(x, data.batch)\n",
        "        x = gmp(x, data.batch)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-aTcYaMQ6es",
        "outputId": "62553584-469b-448b-de9a-df0c56265512"
      },
      "source": [
        "## Test run\n",
        "model = GAT_emb(31,31)\n",
        "temp = model(testrun_batch)\n",
        "\n",
        "print(temp.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O77i8s15Q8-0"
      },
      "source": [
        "## Three layers of sage for three layers of neighbours\n",
        "class SageModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, dim_hidden_sage, out_channels):\n",
        "        super(SageModel,self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, out_channels)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        ## global mean pooling\n",
        "        # x = gap(x, data.batch)\n",
        "        x = gmp(x, data.batch)\n",
        "\n",
        "        return x\n",
        "        "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jn8hlcAQ_vD",
        "outputId": "ba87f03f-4eac-4640-b369-dc3c131a6cf1"
      },
      "source": [
        "## Test run\n",
        "model = SageModel(31, 256, 31)\n",
        "temp = model(testrun_batch)\n",
        "\n",
        "print(temp.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVLbU-cfHQ23"
      },
      "source": [
        "### MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7eipLx8HPpt"
      },
      "source": [
        "class MLPClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, emb_model, dim_input, dim_target, dim_hidden):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        '''\n",
        "        dim_input: dimension of node feature (355)\n",
        "        dim_target: dimension of target y (309)\n",
        "        dim_hidden: number of hidden nodes\n",
        "        max_num_nodes: max number of nodes in subgraphs (not used here)\n",
        "        '''\n",
        "\n",
        "        self.emb = emb_model\n",
        "        self.fc_global = Linear(dim_input, dim_hidden)\n",
        "        self.out = Linear(dim_hidden, dim_target)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = self.emb(data)\n",
        "        x = self.out(F.relu(self.fc_global(x))) \n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFJjCfnOHicf"
      },
      "source": [
        "### Define training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfpdFLCgVsrc"
      },
      "source": [
        "def validate_model_GNN(model, dataloader, convgraph, evel_graph, data_simple, train_with_soft_loss, valid_with_soft_F1):\n",
        "    model.eval()\n",
        "    if train_with_soft_loss:\n",
        "        loss_function = SoftBCEWithLogitsLoss()\n",
        "    else:\n",
        "        loss_function = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        valid_loss = 0\n",
        "        valid_f1s = []\n",
        "        valid_losses = []\n",
        "\n",
        "        count = 1\n",
        "        for batch in dataloader:\n",
        "            output = model(batch)\n",
        "\n",
        "            inputs = data_simple[(count-1)*batch_size:count*batch_size]\n",
        "            labels = batch.y.reshape(batch_size, 12)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if valid_with_soft_F1:\n",
        "                for inp, out, lab in zip(inputs, output, labels):\n",
        "                    y_pred = (out > 0.0).int()\n",
        "                    nearest_gold, best_f1 = evel_graph.get_best_f1_score(inp.data.tolist(), y_pred.data.tolist())\n",
        "                    valid_f1s.append(best_f1)\n",
        "            else:\n",
        "                for out, lab in zip(output, labels):\n",
        "                    valid_f1s.append(f1((out > 0.0).int(), lab).cpu())\n",
        "\n",
        "            if train_with_soft_loss:\n",
        "                loss = loss_function(output.float(),batch.x.float(), convgraph)\n",
        "            else:\n",
        "                loss = loss_function(output.float(), labels.float())   \n",
        "                \n",
        "            valid_losses.append(loss.data.mean().cpu())\n",
        "                \n",
        "    model.train()\n",
        "    return np.mean(valid_losses), np.mean(valid_f1s)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmjCgfoIGG_I"
      },
      "source": [
        "def evaluate_model_GNN(model, dataloader, data_simple, eval_graph, report=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        gold_labels = []\n",
        "        nearest_gold_labels = []\n",
        "        pred_labels = []\n",
        "        soft_f1s = []\n",
        "        \n",
        "        count = 1\n",
        "        for batch in dataloader:\n",
        "            output = model(batch)\n",
        "            # inputs = batch.x\n",
        "            inputs = data_simple[(count-1)*batch_size:count*batch_size]\n",
        "            labels = batch.y.reshape(batch_size, 12)\n",
        "\n",
        "            count +=1\n",
        "\n",
        "            for inp, out, lab in zip(inputs, output, labels):\n",
        "                y_pred = (out > 0.0).int()\n",
        "                pred_labels.append(y_pred.cpu().numpy())\n",
        "                gold_labels.append(lab.cpu().numpy())\n",
        "                # print(torch.tensor(inp).shape)\n",
        "                nearest_gold, best_f1 = eval_graph.get_best_f1_score(inp.data.tolist(), y_pred.data.tolist())\n",
        "                # nearest_gold, best_f1 = get_best_f1_score(inp.data.tolist(), y_pred.data.tolist())\n",
        "                soft_f1s.append(best_f1)\n",
        "                nearest_gold_labels.append(nearest_gold)\n",
        "\n",
        "            \n",
        "        print(\"Hard F-Score (exact match): %.3f\" % f1_score(y_true=np.array(gold_labels, dtype='float32'), y_pred=np.array(pred_labels, dtype='float32'), average='samples'))\n",
        "        print(\"Soft F-Score (best match): %f\" % np.mean(soft_f1s))\n",
        "        if report:\n",
        "            print(classification_report(y_true=np.array(nearest_gold_labels, dtype='float32'),\n",
        "                                        y_pred=np.array(pred_labels, dtype='float32'),\n",
        "                                        target_names=eval_graph.dialog_act_to_idx.keys(), digits=3))\n",
        "    model.train()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmXAswVLICCf"
      },
      "source": [
        "## Define F1 score\n",
        "def f1(y_pred, y_true):\n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "\n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "\n",
        "    epsilon = 1e-7\n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    return 2 * (precision * recall) / (precision + recall + epsilon)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_VrDycUTcqi"
      },
      "source": [
        "def load_checkpoint(model, optimizer, filename='checkpoint'):\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
        "    return model, optimizer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdpfmQ7LH-qH"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuBgI7DlIPQ3"
      },
      "source": [
        "## Model parameters\n",
        "dim_input = torch.tensor(x_train_GNN[1]).shape[1]   ## 31\n",
        "dim_target = torch.tensor(y_train_GNN[1]).shape[0]  ## 12\n",
        "\n",
        "## sage train with max pooling\n",
        "dim_hidden = 64\n",
        "dim_hidden_sage = 64  ## hidden nodes for GraphSage\n",
        "\n",
        "batch_size = 32\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cupLYr8--w9P"
      },
      "source": [
        "train_with_soft_loss = False\n",
        "\n",
        "if train_with_soft_loss:\n",
        "    loss_function = SoftBCEWithLogitsLoss()\n",
        "else:\n",
        "    loss_function = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bfdZN7AH5GB",
        "outputId": "43fbb905-ebde-44f7-ab4d-fc980ab2c067"
      },
      "source": [
        "## GAT training (example run for 5 epochs)\n",
        "emb_model = GAT_emb(dim_input, dim_input)\n",
        "\n",
        "model = MLPClassifier(emb_model, dim_input, dim_target, dim_hidden)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "max_epochs, max_val_f1, patience = 5, 0, 2\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    f1s = []\n",
        "    losses = []\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(batch)\n",
        "        labels = batch.y.reshape(batch_size, 12)\n",
        "        inputs = batch.x\n",
        "        \n",
        "        if train_with_soft_loss:\n",
        "            loss = loss_function(outputs.float(),inputs.float(), train_graph)\n",
        "        else:\n",
        "            loss = loss_function(outputs.float(), labels.float())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.mean().cpu())\n",
        "\n",
        "        for out, lab in zip(outputs, labels):\n",
        "            f1s.append(f1((out > 0.0).float(), lab).cpu())\n",
        "\n",
        "\n",
        "    valid_loss, valid_f1 = validate_model_GNN(model, dev_loader, None, eval_graph_GNN, x_dev_simple, train_with_soft_loss, False)\n",
        "    # valid_loss, valid_f1 = validate_model_GNN_soft(model, dev_loader, x_dev_simple, None, train_with_soft_loss)\n",
        "    # noinspection PyStringFormat\n",
        "    print('[%d/%d] Train Loss: %.3f, Train F1: %.3f, Val Loss: %.3f, Val F1: %.3f,' % (epoch + 1, max_epochs, np.mean(losses), np.mean(f1s), valid_loss, valid_f1))\n",
        "\n",
        "    # Early stopping\n",
        "    if valid_f1 > max_val_f1:\n",
        "        state = {'epoch': epoch + 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), }\n",
        "        torch.save(state, 'checkpoint')\n",
        "\n",
        "        max_val_f1 = valid_f1\n",
        "        max_epochs = 0\n",
        "    else:\n",
        "        max_epochs += 1\n",
        "        if max_epochs >= patience:\n",
        "            model, optimizer = load_checkpoint(model, optimizer)\n",
        "            print(\"Stopped early and went back to Validation f1: %.3f\" % max_val_f1)\n",
        "            break\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/5] Train Loss: 0.497, Train F1: 0.339, Val Loss: 0.462, Val F1: 0.448,\n",
            "[2/0] Train Loss: 0.442, Train F1: 0.430, Val Loss: 0.411, Val F1: 0.447,\n",
            "[3/1] Train Loss: 0.399, Train F1: 0.489, Val Loss: 0.396, Val F1: 0.506,\n",
            "[4/0] Train Loss: 0.375, Train F1: 0.540, Val Loss: 0.393, Val F1: 0.508,\n",
            "[5/0] Train Loss: 0.363, Train F1: 0.562, Val Loss: 0.392, Val F1: 0.513,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHSsjAwXO5Za",
        "outputId": "b0fa36a1-946d-44f3-e72b-1ab3ea675678"
      },
      "source": [
        "### Results for an example run of only 5 epochs. For full training, run for more epochs.\n",
        "print(\"---------------------- DEVELOPMENT SET REPORT --------------------------\")\n",
        "evaluate_model_GNN(model, dev_loader, x_dev_simple ,eval_graph_GNN)\n",
        "\n",
        "print(\"--------------------- DEDUPLICATED TEST SET REPORT -------------------------\")\n",
        "evaluate_model_GNN(model, test_loader, x_test_simple, eval_graph_GNN)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------- DEVELOPMENT SET REPORT --------------------------\n",
            "Hard F-Score (exact match): 0.513\n",
            "Soft F-Score (best match): 0.738276\n",
            "--------------------- DEDUPLICATED TEST SET REPORT -------------------------\n",
            "Hard F-Score (exact match): 0.543\n",
            "Soft F-Score (best match): 0.729661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_UAJPGPN2L"
      },
      "source": [
        "### GraphSAGE training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEbh9fNCPUrN"
      },
      "source": [
        "## Model parameters\n",
        "dim_input = torch.tensor(x_train_GNN[1]).shape[1]   ## 31\n",
        "dim_target = torch.tensor(y_train_GNN[1]).shape[0]  ## 12\n",
        "\n",
        "## sage train with max pooling\n",
        "dim_hidden = 128\n",
        "dim_hidden_sage = 64  ## hidden nodes for GraphSage\n",
        "\n",
        "batch_size = 32"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM0gMjM9Knl3"
      },
      "source": [
        "train_with_soft_loss = False\n",
        "\n",
        "if train_with_soft_loss:\n",
        "    loss_function = SoftBCEWithLogitsLoss()\n",
        "else:\n",
        "    loss_function = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwSxaH4YjXav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1accec7-bd49-4c84-d588-28aed18f5882"
      },
      "source": [
        "## GraphSage training (example run for 5 epochs only)\n",
        "emb_model = SageModel(dim_input, dim_hidden_sage, dim_input)\n",
        "\n",
        "model = MLPClassifier(emb_model, dim_input, dim_target, dim_hidden)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "max_epochs, max_val_f1, patience = 5, 0, 2\n",
        "# max_epochs, max_val_f1, patience = 500, 0, 50\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    f1s = []\n",
        "    losses = []\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(batch)\n",
        "        labels = batch.y.reshape(batch_size, 12)\n",
        "        inputs = batch.x\n",
        "        \n",
        "        if train_with_soft_loss:\n",
        "            loss = loss_function(outputs.float(),inputs.float(), train_graph)\n",
        "        else:\n",
        "            loss = loss_function(outputs.float(), labels.float())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.mean().cpu())\n",
        "\n",
        "        for out, lab in zip(outputs, labels):\n",
        "            f1s.append(f1((out > 0.0).float(), lab).cpu())\n",
        "\n",
        "\n",
        "    valid_loss, valid_f1 = validate_model_GNN(model, dev_loader, None, eval_graph_GNN, x_dev_simple, train_with_soft_loss, False)\n",
        "    # valid_loss, valid_f1 = validate_model_GNN_soft(model, dev_loader, x_dev_simple, None, train_with_soft_loss)\n",
        "    # noinspection PyStringFormat\n",
        "    print('[%d/%d] Train Loss: %.3f, Train F1: %.3f, Val Loss: %.3f, Val F1: %.3f,' % (epoch + 1, max_epochs, np.mean(losses), np.mean(f1s), valid_loss, valid_f1))\n",
        "\n",
        "    # Early stopping\n",
        "    if valid_f1 > max_val_f1:\n",
        "        state = {'epoch': epoch + 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), }\n",
        "        torch.save(state, 'checkpoint')\n",
        "\n",
        "        max_val_f1 = valid_f1\n",
        "        max_epochs = 0\n",
        "    else:\n",
        "        max_epochs += 1\n",
        "        if max_epochs >= patience:\n",
        "            model, optimizer = load_checkpoint(model, optimizer)\n",
        "            print(\"Stopped early and went back to Validation f1: %.3f\" % max_val_f1)\n",
        "            break\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/5] Train Loss: 0.504, Train F1: 0.291, Val Loss: 0.478, Val F1: 0.384,\n",
            "[2/0] Train Loss: 0.474, Train F1: 0.377, Val Loss: 0.467, Val F1: 0.443,\n",
            "[3/0] Train Loss: 0.453, Train F1: 0.435, Val Loss: 0.443, Val F1: 0.429,\n",
            "[4/1] Train Loss: 0.423, Train F1: 0.474, Val Loss: 0.421, Val F1: 0.445,\n",
            "[5/0] Train Loss: 0.403, Train F1: 0.512, Val Loss: 0.410, Val F1: 0.454,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDoFUPaT8FEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c2ff8e-3170-4ecb-a8e4-6a1b78e42dcb"
      },
      "source": [
        "### Results for an example run of only 5 epochs. For full training, run for more epochs.\n",
        "print(\"---------------------- DEVELOPMENT SET REPORT --------------------------\")\n",
        "evaluate_model_GNN(model, dev_loader, x_dev_simple ,eval_graph_GNN)\n",
        "\n",
        "print(\"--------------------- DEDUPLICATED TEST SET REPORT -------------------------\")\n",
        "evaluate_model_GNN(model, test_loader, x_test_simple, eval_graph_GNN)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------- DEVELOPMENT SET REPORT --------------------------\n",
            "Hard F-Score (exact match): 0.454\n",
            "Soft F-Score (best match): 0.639206\n",
            "--------------------- DEDUPLICATED TEST SET REPORT -------------------------\n",
            "Hard F-Score (exact match): 0.463\n",
            "Soft F-Score (best match): 0.623753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPHOERxHqCoZ"
      },
      "source": [
        "### Simple Search Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u46a9gulezS"
      },
      "source": [
        "train_with_soft_loss = False\n",
        "if train_with_soft_loss:\n",
        "    loss_function = SoftBCEWithLogitsLoss()\n",
        "else:\n",
        "    loss_function = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEpBcHkjUcXM"
      },
      "source": [
        "def find_nearest_node(node, node_seen, method):\n",
        "    node = np.array(node).reshape(1,-1)\n",
        "    dist = cdist(node, node_seen, method)\n",
        "\n",
        "    idx = np.argmin(dist)\n",
        "    nearest_node = node_seen[idx]\n",
        "\n",
        "    return list(nearest_node)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4tKizO2stYy"
      },
      "source": [
        "def simple_search(dataset, data_seen, graph, method):\n",
        "    y_pred = []\n",
        "    ## Count how many nodes not seen in search graph\n",
        "    count_oov = 0\n",
        "    for data in dataset:\n",
        "        node = str(list(data))\n",
        "        if graph.has_node(node):\n",
        "            dialog_acts = [(graph[node][t], t) for t in graph[node]]\n",
        "            dialog_acts.sort(key=lambda t: t[0]['probability'], reverse=True)\n",
        "            for dialog_act in dialog_acts[:1]:\n",
        "                y = eval(dialog_act[1])[19:]\n",
        "                y_pred.append(y)\n",
        "        else:\n",
        "            count_oov += 1\n",
        "\n",
        "            data_hat = find_nearest_node(data, data_seen, method)\n",
        "            data_hat = list(map(int, data_hat))\n",
        "            node = str(data_hat)\n",
        "            if graph.has_node(node) == False:\n",
        "                print('incorrect')\n",
        "            dialog_acts = [(graph[node][t], t) for t in graph[node]]\n",
        "            dialog_acts.sort(key=lambda t: t[0]['probability'], reverse=True)\n",
        "            for dialog_act in dialog_acts[:1]:\n",
        "                y = eval(dialog_act[1])[19:]\n",
        "                y_pred.append(y)\n",
        "    return y_pred, count_oov\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l21n9TFlheU",
        "outputId": "85b6aba4-9467-4fc8-d7b8-388bc60f050d"
      },
      "source": [
        "use_search = True\n",
        "## method_list = {'euclidean', 'jaccard'}\n",
        "method = 'jaccard'\n",
        "\n",
        "if use_search == True:\n",
        "    f1s_test = []\n",
        "    ## Test set results\n",
        "    outputs, test_oov = simple_search(x_test, x_seen_train, train_graph, method)\n",
        "\n",
        "    outputs = torch.tensor(outputs).float()\n",
        "    labels = torch.tensor(y_test_GNN).float()\n",
        "\n",
        "    inputs = x_test_simple\n",
        "\n",
        "    soft_f1s = []\n",
        "\n",
        "    if train_with_soft_loss:\n",
        "        loss_test = loss_function(outputs, x_test, train_graph_GNN)\n",
        "    else:\n",
        "        loss_test = loss_function(outputs, labels)\n",
        "\n",
        "    for out, lab in zip(outputs, labels):\n",
        "        f1s_test.append(f1((out > 0.0).float(), lab).cpu())\n",
        "\n",
        "    for inp, out, lab in zip(inputs, outputs, labels):\n",
        "        y_pred = (out > 0.0).int()\n",
        "        nearest_gold, best_f1 = eval_graph_GNN.get_best_f1_score(inp.data.tolist(), y_pred.data.tolist())\n",
        "\n",
        "        soft_f1s.append(best_f1)\n",
        "\n",
        "    print('Testset result of Simple Search Method:', 'Test loss:', loss_test.data.mean().cpu(), 'Test F1:', np.mean(f1s_test))\n",
        "    print('Number of test nodes not found in the search graph:', test_oov)\n",
        "    print('Testset Soft F1: ', np.mean(soft_f1s))\n",
        "\n",
        "    ##################################\n",
        "    ## Dev set results\n",
        "    f1s_dev = []\n",
        "    soft_f1s_dev = []\n",
        "\n",
        "    outputs_dev, dev_oov = simple_search(x_dev, x_seen_train, train_graph, method)\n",
        "\n",
        "    outputs_dev = torch.tensor(outputs_dev).float()\n",
        "    labels_dev = torch.tensor(y_dev_GNN).float()\n",
        "\n",
        "    inputs_dev = x_dev_simple\n",
        "\n",
        "    if train_with_soft_loss:\n",
        "        loss_dev = loss_function(outputs_dev, x, train_graph_GNN)\n",
        "    else:\n",
        "        loss_dev = loss_function(outputs_dev, labels_dev)\n",
        "\n",
        "    for out, lab in zip(outputs_dev, labels_dev):\n",
        "        f1s_dev.append(f1((out > 0.0).float(), lab).cpu())\n",
        "\n",
        "    for inp, out, lab in zip(inputs_dev, outputs_dev, labels_dev):\n",
        "        y_pred = (out > 0.0).int()\n",
        "        nearest_gold, best_f1 = eval_graph_GNN.get_best_f1_score(inp.data.tolist(), y_pred.data.tolist())\n",
        "\n",
        "        soft_f1s_dev.append(best_f1)\n",
        "\n",
        "    print('Devset result of Simple Search Method:', 'Valid loss:', loss_dev.data.mean().cpu(), 'Valid F1:', np.mean(f1s_dev))\n",
        "    print('Number of dev nodes not found in the search graph:', dev_oov)\n",
        "    print('Testset Soft F1: ', np.mean(soft_f1s_dev))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testset result of Simple Search Method: Test loss: tensor(0.6759) Test F1: 0.6571501\n",
            "Number of test nodes not found in the search graph: 90\n",
            "Testset Soft F1:  0.9692056277056277\n",
            "Devset result of Simple Search Method: Valid loss: tensor(0.6943) Valid F1: 0.60050243\n",
            "Number of dev nodes not found in the search graph: 33\n",
            "Testset Soft F1:  0.9752465483234715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuazMi44ZM1M"
      },
      "source": [
        "### Sequential models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1_lvNMXZLkr"
      },
      "source": [
        "### Data loader for LinearData\n",
        "x_train, y_train = train_graph_GNN.generate_standard_data(unique=False)\n",
        "x_dev, y_dev = dev_graph_GNN.generate_standard_data(unique=False)\n",
        "x_test, y_test = test_graph_GNN.generate_standard_data(unique=True)\n",
        "\n",
        "params = {'batch_size': 32, 'shuffle': True}\n",
        "\n",
        "training_set = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
        "validation_set = TensorDataset(torch.tensor(x_dev, dtype=torch.float32), torch.tensor(y_dev, dtype=torch.float32))\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
        "no_dupl_test_set = TensorDataset(torch.tensor(x_test, dtype=torch.float32),\n",
        "                                  torch.tensor(y_test, dtype=torch.float32))\n",
        "no_dupl_test_generator = torch.utils.data.DataLoader(no_dupl_test_set, **params)\n",
        "\n",
        "### OR USE MEANDATA below\n",
        "### Data loader for MeanData\n",
        "# training_set = TensorDataset(torch.tensor(x_train_simple, dtype=torch.float32), torch.tensor(y_train_GNN, dtype=torch.float32))\n",
        "# training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
        "# validation_set = TensorDataset(torch.tensor(x_dev_simple, dtype=torch.float32), torch.tensor(y_dev_GNN, dtype=torch.float32))\n",
        "# validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
        "# no_dupl_test_set = TensorDataset(torch.tensor(x_test_simple, dtype=torch.float32),\n",
        "#                                   torch.tensor(y_test_GNN, dtype=torch.float32))\n",
        "# no_dupl_test_generator = torch.utils.data.DataLoader(no_dupl_test_set, **params)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1uLanNfoUAQ",
        "outputId": "390a16b1-f049-415d-bc9a-cde1668a1a26"
      },
      "source": [
        "x_train_simple[1].shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb1ac_PKadlY"
      },
      "source": [
        "## Model parameters\n",
        "dim_input = torch.tensor(x_train_GNN[1]).shape[1]   ## 31\n",
        "dim_target = torch.tensor(y_train_GNN[1]).shape[0]  ## 12\n",
        "\n",
        "## sage train with max pooling\n",
        "dim_hidden = 256\n",
        "dim_hidden_sage = 128  ## hidden nodes for GraphSage\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yv-e35cau7m"
      },
      "source": [
        "train_with_soft_loss = False\n",
        "\n",
        "if train_with_soft_loss:\n",
        "    loss_function = SoftBCEWithLogitsLoss()\n",
        "else:\n",
        "    loss_function = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4JJqtjta6EO"
      },
      "source": [
        "def load_checkpoint(model, optimizer, filename='checkpoint'):\n",
        "    checkpoint = torch.load(filename)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
        "    return model, optimizer"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wOvOIWZcXv",
        "outputId": "95fe5bfc-bb8c-439b-fa92-fdff8c8ff97d"
      },
      "source": [
        "## Example run for sequential model. For training, need to run for more epochs\n",
        "\n",
        "## Choose a classifier (LSTM, MLP, Bi-LSTM, RNN)\n",
        "\n",
        "classifier = LSTM_model(dim_input, dim_target)\n",
        "# classifier = Perceptron(dim_input, dim_target)\n",
        "# classifier = BiLSTM(dim_input, dim_hidden, dim_target)\n",
        "# classifier = RNN_model(dim_input, dim_target, dim_hidden)\n",
        "\n",
        "optimizer = optim.RMSprop(classifier.parameters())\n",
        "max_epochs, max_val_f1, patience = 5, 0, 2\n",
        "\n",
        "print(\"Example run for sequential model. For training, need to run for more epochs.\")\n",
        "\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    f1s = []\n",
        "    losses = []\n",
        "    for inputs, labels in training_generator:\n",
        "        classifier.zero_grad()\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        output = classifier(inputs)\n",
        "        if train_with_soft_loss:\n",
        "            loss = loss_function(output, inputs, train_graph_GNN)\n",
        "        else:\n",
        "            loss = loss_function(output, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.data.mean().cpu())\n",
        "\n",
        "        for out, lab in zip(output, labels):\n",
        "            f1s.append(f1((out > 0.0).float(), lab).cpu())\n",
        "\n",
        "    valid_loss, valid_f1 = validate_model(classifier, validation_generator, dev_graph_GNN, train_with_soft_loss, device)\n",
        "    # noinspection PyStringFormat\n",
        "    print('[%d/%d] Train Loss: %.3f, Train F1: %.3f, Val Loss: %.3f, Val F1: %.3f,' % (epoch + 1, 50, np.mean(losses), np.mean(f1s), valid_loss, valid_f1))\n",
        "\n",
        "    # Early stopping\n",
        "    if valid_f1 > max_val_f1:\n",
        "        state = {'epoch': epoch + 1, 'state_dict': classifier.state_dict(), 'optimizer': optimizer.state_dict(), }\n",
        "        torch.save(state, 'checkpoint')\n",
        "\n",
        "        max_val_f1 = valid_f1\n",
        "        max_epochs = 0\n",
        "    else:\n",
        "        max_epochs += 1\n",
        "        if max_epochs >= patience:\n",
        "            classifier, optimizer = load_checkpoint(classifier, optimizer)\n",
        "            print(\"Stopped early and went back to Validation f1: %.3f\" % max_val_f1)\n",
        "            break\n",
        "\n",
        "print(\"---------------------- DEVELOPMENT SET REPORT --------------------------\")\n",
        "evaluate_model(classifier, validation_generator, eval_graph_GNN, device)\n",
        "\n",
        "print(\"--------------------- DEDUPLICATED TEST SET REPORT -------------------------\")\n",
        "evaluate_model(classifier, no_dupl_test_generator, eval_graph_GNN, device)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example run for sequential model. For training, need to run for more epochs.\n",
            "[1/50] Train Loss: 0.476, Train F1: 0.436, Val Loss: 0.401, Val F1: 0.468,\n",
            "[2/50] Train Loss: 0.371, Train F1: 0.549, Val Loss: 0.394, Val F1: 0.569,\n",
            "[3/50] Train Loss: 0.344, Train F1: 0.598, Val Loss: 0.361, Val F1: 0.555,\n",
            "[4/50] Train Loss: 0.319, Train F1: 0.642, Val Loss: 0.362, Val F1: 0.562,\n",
            "=> loaded checkpoint 'checkpoint' (epoch 2)\n",
            "Stopped early and went back to Validation f1: 0.569\n",
            "---------------------- DEVELOPMENT SET REPORT --------------------------\n",
            "Hard F-Score (exact match): 0.569\n",
            "Soft F-Score (best match): 0.806874\n",
            "--------------------- DEDUPLICATED TEST SET REPORT -------------------------\n",
            "Hard F-Score (exact match): 0.586\n",
            "Soft F-Score (best match): 0.785284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGRj9LLUpIYu",
        "outputId": "45db9762-e0f8-4b1e-d11a-379751d42c71"
      },
      "source": [
        "## MLP\n",
        "VHF = [0.650, 0.656, 0.657, 0.655, 0.649]\n",
        "VSF = [0.915413, 0.909068, 0.876200, 0.901425, 0.888876]\n",
        "\n",
        "THF = [0.702, 0.707, 0.702, 0.704 , 0.690]\n",
        "TSF = [0.906208, 0.923331, 0.907678, 0.911401, 0.903732]\n",
        "\n",
        "print(np.mean(VHF), np.mean(VSF), np.mean(THF), np.mean(TSF))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6534000000000001 0.8981964 0.7009999999999998 0.9104700000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO40b6C4kbHX",
        "outputId": "8095d03a-d8e9-463a-8098-8e83fa108191"
      },
      "source": [
        "## RNN\n",
        "VHF = [0.299, 0.497, 0.441, 0.337, 0.357]\n",
        "VSF = [0.426390, 0.755078, 0.689185, 0.578410, 0.599399]\n",
        "\n",
        "THF = [0.244, 0.487, 0.424, 0.327, 0.359]\n",
        "TSF = [0.359018, 0.684507,0.610320, 0.539102, 0.553551]\n",
        "print(np.mean(VHF), np.mean(VSF), np.mean(THF), np.mean(TSF))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3862 0.6096923999999999 0.36819999999999997 0.5492996000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLXauJDZjON4",
        "outputId": "e8f368b6-e9bd-4d29-8bfa-bec4d821ed25"
      },
      "source": [
        "## Bi-LSTM\n",
        "VHF = [0.640, 0.664, 0.651, 0.663, 0.652]\n",
        "VSF = [0.925991, 0.935895, 0.914295, 0.881976, 0.936639]\n",
        "\n",
        "THF = [0.671, 0.713, 0.685, 0.686, 0.699]\n",
        "TSF = [0.907320, 0.931169, 0.904885, 0.892608, 0.935338]\n",
        "\n",
        "print(np.mean(VHF), np.mean(VSF), np.mean(THF), np.mean(TSF))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6540000000000001 0.9189592000000001 0.6908 0.914264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyk0wd0r8Ues",
        "outputId": "8f16d58d-2d9d-47fd-8ddc-a1e496b7005a"
      },
      "source": [
        "## LSTM\n",
        "VHF = [0.657, 0.656, 0.640, 0.667, 0.662]\n",
        "VSF = [0.892167, 0.916457, 0.899351, 0.927221, 0.921875]\n",
        "\n",
        "THF = [0.684, 0.697, 0.690, 0.722, 0.704]\n",
        "TSF = [0.892062, 0.906511, 0.900117, 0.932719, 0.915623]\n",
        "print(np.mean(VHF), np.mean(VSF), np.mean(THF), np.mean(TSF))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6564 0.9114142000000001 0.6994 0.9094064\n"
          ]
        }
      ]
    }
  ]
}